
# **ğŸ“˜ Project Documentation**

This project provides a **prediction API** that analyzes client purchase patterns from a **remote MySQL database (via SSH tunnel)**.
The API runs on **FastAPI** and exposes endpoints at:
â¡ï¸ [http://localhost:8000/docs](http://localhost:8000/docs)

---

# **ğŸš€ Getting Started**

## **1. Clone the Repository**

```bash
git clone <your-repo-url>
cd <your-project-folder>
```

---

## **2. Install Dependencies**

Make sure you have **Python 3.10+** installed.

```bash
pip install -r requirements.txt
```

---

## **3. Configure Environment**

Create a `.env` file with your **SSH and MySQL credentials**:

```dotenv
SSH_HOST=<remote_host>
SSH_PORT=<ssh_port>
SSH_USER=<ssh_user>
SSH_PASSWORD=<ssh_password>

DB_HOST=<db_host>
DB_PORT=<db_port>
DB_USER=<db_user>
DB_PASSWORD=<db_password>
DB_NAME=<db_name>
```

> âš ï¸ Incorrect values here will prevent the API from connecting to the database.

---

## **4. Run the API Server**

```bash
uvicorn backend.main:app --reload
```

Open API docs:
â¡ï¸ [http://localhost:8000/docs](http://localhost:8000/docs)

---

# **ğŸ“‚ Project Structure**

```
project/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ remote_db.py         # Remote MySQL connector (via SSH tunnel)
â”‚   â””â”€â”€ prediction.py        # Main prediction logic (cycle analysis)
â”‚
â”œâ”€â”€ requests/
â”‚   â””â”€â”€ rq.py                # Service layer: SQL queries using RemoteMySQL
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ predict.py           # FastAPI router for predictions
â”‚
â”œâ”€â”€ .env                     # Environment variables (SSH + SQL)
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ backend/main.py          # FastAPI app entrypoint
```

---

# **ğŸ”Œ How It Works**

### **1. `core/remote_db.py` â€“ Remote DB Connector**

* Opens an SSH tunnel
* Connects to remote MySQL
* Runs `.query()` and `.execute()`
* Returns results as Python dictionaries

Used everywhere you need database access.

---

### **2. `requests/rq.py` â€“ Service Layer**

* Runs SQL queries using `RemoteMySQL`
* Fetches sales, clients, goods data
* Returns structured datasets for prediction

Think of it as the **bridge between DB and prediction engine**.

---

### **3. `core/prediction.py` â€“ Prediction Logic**

* Calculates purchase cycles
* Scores variance & confidence
* Predicts next order date & amount
* Classifies patterns (e.g., highly regular)
* Filters out irrelevant data

This is the **brain of the system**.

---

### **4. `api/predict.py` â€“ API Router**

**Endpoint:**

```
GET /predict/
```

* Accepts filters (`min_requirements`, `confidence_threshold`)
* Fetches DB data via `rq.py`
* Runs prediction logic from `core/prediction.py`
* Returns JSON results

All output is visible in Swagger UI.

---

# **ğŸ“Š API Output**

For each **client + product** pair, the system detects:

* Purchase frequency
* Pattern stability
* Last purchase date
* Predicted next purchase date
* Predicted amount
* Confidence score

Useful for:

* CRM integration
* Sales forecasting
* Manager reminders
* Detecting lost clients

---

# **ğŸ”§ Example Output**

```json
{
  "client_id": 17834,
  "client_name": "Customer 1",
  "goods_id": 2726,
  "goods_name": "Chemical A",
  "last_requirement_date": "2025-06-23",
  "predicted_next_purchase_date": "2025-11-10",
  "avg_interval_days": 140,
  "predicted_amount": 33.33,
  "confidence_score": 0.95,
  "pattern_consistency": "highly_regular"
}
```

---

# **ğŸ’¡ How to Run (Step by Step)**

1. Clone repo & navigate:

```bash
git clone <your-repo-url>
cd <your-project-folder>
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Configure `.env` with SSH + MySQL credentials.

4. Start the API:

```bash
uvicorn main:app --reload
```

5. Test endpoints in Swagger UI:
   â¡ï¸ [http://localhost:8000/docs](http://localhost:8000/docs)

6. Optional: Use Postman or any HTTP client to query `/predict/`.

